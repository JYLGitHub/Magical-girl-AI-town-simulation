// ğŸ“ llm.js íŒŒì¼ì´ ìˆë‹¤ë©´ ì•„ë˜ ë‚´ìš©ìœ¼ë¡œ êµì²´í•´ì£¼ì„¸ìš”.

const { GoogleGenerativeAI } = require('@google/generative-ai');
const OpenAI = require('openai');

const gemini = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ê°„ë‹¨í•œ sleep í•¨ìˆ˜
const sleep = (ms) => new Promise(resolve => setTimeout(resolve, ms));

async function callLLM(prompt, provider = 'gemini') {
    // â­ í•µì‹¬ ì¶”ê°€: ìµœëŒ€ 3ë²ˆê¹Œì§€ ì¬ì‹œë„
    for (let attempt = 1; attempt <= 3; attempt++) {
        try {
            if (provider === 'gemini') {
                const model = gemini.getGenerativeModel({ model: 'gemini-1.5-flash-latest' });
                const result = await model.generateContent(prompt);
                return result.response.text();
            } else { // gpt
                const completion = await openai.chat.completions.create({
                    model: "gpt-4o-mini",
                    messages: [{ role: "user", content: prompt }],
                });
                return completion.choices[0].message.content;
            }
        } catch (error) {
            // 503 ê³¼ë¶€í•˜ ì—ëŸ¬ì¼ ê²½ìš°ì—ë§Œ ì¬ì‹œë„
            if (error.status === 503 && attempt < 3) {
                console.log(`[LLM ì¬ì‹œë„] ${provider} API ê³¼ë¶€í•˜. ${attempt}ë²ˆì§¸ ì¬ì‹œë„...`);
                await sleep(1500); // 1.5ì´ˆ ëŒ€ê¸°
            } else {
                console.error(`[LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜] Provider: ${provider}`, error);
                // ìµœì¢…ì ìœ¼ë¡œ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ë¥¼ throwí•˜ì—¬ ìƒìœ„ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                throw new Error(`${provider} API Error: ${error.message}`);
            }
        }
    }
}

module.exports = { callLLM };